---
title: "Projekt-Dokumentation: Workflows zur Texterkennung mit Tesseract und OCR4all am Beispiel von Super-8-Filmkatalogen des Anbieters UFA"
sidebar: false
---
## Teil I: Tesseract Standardmodell &amp; Bild-Vorbearbeitung 

### About: Textdigitalisierung, OCR und Tesseract
1. Einstieg: [Möglichkeiten der Textdigitalisierung (ForText)](https://fortext.net/routinen/methoden/moeglichkeiten-der-textdigitalisierung) 
2. Einstieg: [OCR und Tesseract](https://nanonets.com/blog/ocr-with-tesseract/)
5. Überblick: [Tesseract Wiki](https://wiki.ubuntuusers.de/tesseract-ocr/)

### 1. Vergleich: Parameter (Stichprobe)

#### Pagesegmentation &amp; Engine: Beispiel KaUf_77_2_0006

```%Tesseract%/tesseract KaUf_77_2_0006.jpg KaUf_77_2_0006-out -l deu [psm][oem] get.images```


![](https://s3.hedgedoc.org/demo/uploads/786ce81d-3b2f-4f83-be6a-5d8141d56c9f.png)

#### Page Segmentation (-- psm):
Quelle: [Tesseract Page Segmentation](https://pyimagesearch.com/2021/11/15/tesseract-page-segmentation-modes-psms-explained-how-to-improve-your-ocr-accuracy/)

**0**	Nur Ausrichtung und Skript-Erkennung: Metainformationen (Rotation, Schrift, etc.)
**1**	**Automatische Seitensegmentierung mit OSD**
**2**	Automatische Seitensegmentierung, aber keine OSD, oder Texterkennung (OCR)
**3**	**Vollständige automatische Seitensegmentierung, aber keine OSD (Default)**
**4**	Behandelt die Vorlage als eine einzelne Textspalte mit unterschiedlichen Zeichengrößen
**5**	Behandelt die Vorlage als einzelnen einheitlichen Textblock im Blocksatz
**6**	Behandelt die Vorlage als einheitlichen Textblock
**7**	Behandelt das Bild als einzelne Textzeile
**8**	Behandelt das Bild als einzelnes Wort
**9**	Behandelt das Bild als einzelnes, im Kreis geschriebenes Wort
**10**	Behandelt das Bild als einzelnes Zeichen
**11**	Reiner Text, findet so viel Text wie irgend möglich, ohne spezielle Richtung oder Reihenfolge
**12**	Reiner Text, mit OSD (siehe Option 0)
**13**	&#34;Raw line&#34; - behandelt das Bild als einzelen Textzeile, ohne weitere tesseract-spezifische Verarbeitungen anzuwenden


#### OCR engine mode (-- oem):
0    nur Legacy Engine
1    nur Neural Nets LSTM Engine
**2    Legacy + LSTM Engines**
**3    Default, nach Verfügbarkeit**

#### Configuration Variables (- c)
```%Tesseract%/tesseract --print-parameters &gt; paramters.txt```

---

### 2. Vergleich: Bildvorverarbeitung (UFA .tif komplett)

```for %i in (D:\UFA_SDF\KaUf_*\TIFF\*.tif) do tesseract.exe &#34;%i&#34; &#34;%i-out&#34; -l deu get.images```

```move *.processed.tif C:\[*]\Tesseract-OCR\processed_images```

#### Read
1. [Tesseract Doku Bildvorverarbeitung](https://tesseract-ocr.github.io/tessdoc/ImproveQuality#image-processing)
2. [Otsu&#39;s Treshholding Technique](https://learnopencv.com/otsu-thresholding-with-opencv/)
3. [Tesseract Finetuning](https://www.statworx.com/content-hub/blog/finetuning-von-tesseract-ocr-fuer-deutsche-rechnungen/)

#### Problematik Vorverarbeitung: Beispiel KaUf_77_2_0040

![KaUf_77_2_0040](https://s3.hedgedoc.org/demo/uploads/e90c89aa-97da-4ccd-9979-5dd763fcefdf.png)

#### Zusammengefasst
Grundsätzlich muss je nach Eingabebild der zu verwendende Schwellenwertalgorithmus ausgewählt werden. **Tesseract verwendet die Otsu-Methode für die [Schwellenwertberechnung](https://de.wikipedia.org/wiki/Schwellenwertverfahren)**, da die Eingabe in Tesseract für die Textextraktion im Allgemeinen homogene Bilder aufweist. 

Die **globale Schwellenwertmethode** ist sinnvoll, wenn der Hintergrund keine lokalen Schwankungen im Verhältnis zur Intensität des Vordergrunds aufweist. Ein **lokaler Schwellenwert ist hingegen notwendig**, wenn die Intensitätsunterschiede zwischen Hintergrund und Ziel lokal variieren.

Obwohl Tesseract die Otsu-Methode (globale Schwellenwertbildung) für die Binarisierung verwendet, kann man Bilder mit lokalen Schwellenwertmethoden vorverarbeiten, um ein besseres Ergebnis von Tesseract zu erhalten.

#### Andere Optionen

Seit Tesseract 5.0.0 zwei neue [Leptonica](http://leptonica.org/local-sources.html)-basierte Binarisierungsmethoden: *Adaptive Otsu* und *Sauvola*. 

```tesseract --print-parameters | grep thresholding_```

**relevante konfigurierbare Parameter:**


| parameter | default | modification |
| -------- | -------- | -------- |
| thresholding_method     | 0     |   0=Otsu, 1=LeptonicaOtsu, 2=Sauvola|
| invert_threshold  | 0.7 | For lines with a mean confidence below this value, OCR is also tried with an inverted image |
| thresholding_debug | 0 | Debug the thresholding process |
| thresholding_window_size | 0.33 | Window size for measuring local statistics (to be multiplied by image DPI). This parameter is used by the Sauvola thresholding method |

**weitere:**
```thresholding_kfactor``` 
0.34 | Factor for reducing threshold due to variance. This parameter is used by the Sauvola thresholding method. Normal range: 0.2-0.5

```thresholding_tile_size``` 
0.33 | Desired tile size (to be multiplied by image DPI). This parameter is used by the LeptonicaOtsu thresholding method

```thresholding_smooth_kernel_size``` 
0 | Size of convolution kernel applied to threshold array (to be multiplied by image DPI). Use 0 for no smoothing. This parameter is used by the LeptonicaOtsu thresholding method

```thresholding_score_fraction``` 
0.1 | Fraction of the max Otsu score. This parameter is used by the LeptonicaOtsu thresholding method. For standard Otsu use 0.0, otherwise 0.1 is recommended


**außerdem möglich:**

- ImageJ Auto Threshold (java) 
- OpenCV Image Thresholding (python) 
- scikit-image Thresholding Dokumentation (python)

--- 

### 3. Vorverarbeitung (Auswahl)

#### Worst of: Tesseract Preprocessing

![](https://s3.hedgedoc.org/demo/uploads/55d1c8b1-d66c-40b7-8bc1-43681c36cf11.jfif)

![](https://s3.hedgedoc.org/demo/uploads/fc6365bd-cd89-40b0-98ef-bb3585a8e0b2.png)


#### Read
1. [ImageMagick](https://imagemagick.org/)
2. [Grayscale](https://en.wikipedia.org/wiki/Grayscale)
3. [Saturation](https://en.wikipedia.org/wiki/Colorfulness)


#### Grayscaling
[Grayscale Method](https://imagemagick.org/script/command-line-options.php#grayscale)
```for %i in () do magick &#34;%i&#34; -colorspace Gray &#34;%i-grayscale&#34;```
![](https://s3.hedgedoc.org/demo/uploads/455fc728-d2b3-4dea-89c9-99ee594b6e62.png)


#### Saturation
[Brightness Modulation](https://imagemagick.org/script/command-line-options.php#modulate)
```for %i in () do magick &#34;%i&#34; -modulate 100,0 &#34;%i-sat0&#34;```
![](https://s3.hedgedoc.org/demo/uploads/fabe4a48-3ffe-46e2-b5a3-8f2daa322caf.png)

---

### 4. Threshold Red 80%

#### Threshold
[Threshold Value](https://imagemagick.org/script/command-line-options.php#threshold)

#### Stichprobe KaUf_76_2_0038
```magick KaUf_76_2_0038.tif -channel red -threshold 80% KaUf_76_2_0038-out.tif```
```magick KaUf_76_2_0038-out.tif -modulate 100,0 KaUf_76_2_0038-outfinal.tif```
```%Tesseract%/tesseract KaUf_76_2_0038-outfinal.tif final.tif -l deu get.images```

![](https://s3.hedgedoc.org/demo/uploads/6fe78f90-b8e9-4936-babe-853a8dd9968d.png)

#### Auswahl
```for %i in (C:\Users\[*]\Desktop\tess_test_config\*.tif) do magick &#34;%i&#34; -channel red -threshold 80% &#34;%i-thresh.tif&#34;```
```for %i in (C:\Users\[*]\Desktop\tess_test_config\threshold\*-thresh.tif) do magick &#34;%i&#34; -modulate 100,0 &#34;%i-out.tif&#34;```
```for %i in (C:\Users\[*]\Desktop\tess_test_config\threshold\*-out.tif) do %Tesseract%\tesseract &#34;%i&#34; &#34;%i-threshR80&#34; -l deu get.images```

---

### 5. Workflow: Magick &amp; Tesseract
**1. Threshold Red 80%**
&gt;Allen Pixelwerten (genauer gesagt, die mit -channel eingestellten Kanäle), die den angegebenen Schwellenwert überschreiten, wird der maximale Kanalwert zugewiesen, während alle anderen Werte den minimalen Wert erhalten.

**Weniger Verluste ohne Farb-Threshold Parameter?**

**2. Saturation 0%**
&gt;Die Sättigung bestimmt die Menge an Farbe in einem Bild. Ein Wert von 0 ergibt beispielsweise ein Graustufenbild, während ein hoher Wert wie 200 eine sehr bunte, &#34;cartoonhafte&#34; Farbe erzeugt.

**3. Tesseract**
&gt;Die Standardeinstellungen von Tesseract mit dem [besten Sprachmodell ](https://github.com/tesseract-ocr/tessdata_best) Deutsch (im LSTM Engine Mode) liefern eine sehr genaue Ausgabe.

### Extras: Weitere Verbesserungen?
#### Read
[Improving the Recognition Accuracy of Tesseract-OCR Engine on Nepali Text Images via Preprocessing](https://zenodo.org/record/4361896#.YzLJddjP1PY)

#### Genauigkeit bestimmen
**Funktionen Tesseract?**
[Recap Tesseract](https://github.com/tesseract-ocr/tesseract/blob/main/doc/tesseract.1.asc)

[PyTesseract](https://pypi.org/project/pytesseract/)
&gt;**image_to_data** Returns result containing box boundaries, confidences, and other information. Requires Tesseract 3.05+. For more information, please check the Tesseract TSV documentation

[Image Processing](https://stackoverflow.com/questions/20831612/getting-the-bounding-box-of-the-recognized-words-using-python-tesseract)
&gt; Among the data returned by pytesseract.image_to_data():
left is the distance from the upper-left corner of the bounding box, to the left border of the image.
&gt; top is the distance from the upper-left corner of the bounding box, to the top border of the image.
width and height are the width and height of the bounding box.
&gt; conf is the model&#39;s confidence for the prediction for the word within that bounding box. If conf is -1, that means that the corresponding bounding box contains a block of text, rather than just a single word.
&gt; The bounding boxes returned by pytesseract.image_to_boxes() enclose letters so I believe pytesseract.image_to_data() is what you&#39;re looking for.

---

## Teil II: Training

### About: Tesseract Training
1. [Tesseract Development Documentation](https://tesseract-ocr.github.io/tessapi/5.x/index.html)
2. [Training Tesseract on Custom Data](https://nanonets.com/blog/ocr-with-tesseract/#training-tesseract-on-custom-data)
3. [Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
4. [Train Tesseract Model from Scratch](https://www.endpointdev.com/blog/2018/07/training-tesseract-models-from-scratch/)
5. [Train Tesseract OCR in Python](https://www.projectpro.io/article/how-to-train-tesseract-ocr-python/561)
6. [OCR Tesseract Training Data](https://pretius.com/blog/ocr-tesseract-training-data/)
7. [Train Tesseract HTR](https://towardsdatascience.com/train-a-custom-tesseract-ocr-model-as-an-alternative-to-google-vision-ocr-for-reading-childrens-db8ba41571c8)
8. [How to train Tesseract](https://tesseract-ocr.github.io/tessdoc/tess4/TrainingTesseract-4.00.html9)

#### Zusammengefasst
&gt; Neural networks require significantly more training data and train a lot slower than base Tesseract. For Latin-based languages, the existing model data provided has been trained on about 400000 textlines spanning about 4500 fonts. For other scripts, not so many fonts are available, but they have still been trained on a similar number of textlines. Instead of taking a few minutes to a couple of hours to train, Tesseract 4.00 takes a few days to a couple of weeks. Even with all this new training data, you might find it inadequate for your particular problem, and therefore you are here wanting to retrain it.

Quelle: [How to train Tesseract](https://tesseract-ocr.github.io/tessdoc/tess4/TrainingTesseract-4.00.html) 

---

### 1. Optionen

&gt; **Fine tune.** Starting with an existing trained language, train on your specific additional data. This may work for problems that are close to the existing training data, but different in some subtle way, like a particularly unusual font. May work with even a small amount of training data.
&gt; **Cut off the top layer (or some arbitrary number of layers) from the network and retrain a new top layer using the new data.** If fine tuning doesn’t work, this is most likely the next best option. Cutting off the top layer could still work for training a completely new language or script, if you start with the most similar looking script.
&gt; **Retrain from scratch.** This is a daunting task, unless you have a very representative and sufficiently large training set for your problem. If not, you are likely to end up with an over-fitted network that does really well on the training data, but not on the actual data.

Quelle: [How to train Tesseract](https://tesseract-ocr.github.io/tessdoc/tess4/TrainingTesseract-4.00.html)

---

### 2. Training mit tesseract v5.2:
&gt; Training with tesstrain.sh bash scripts is unsupported/abandoned for Tesseract 5. Please use python scripts from tesstrain repo for training.

Quelle: [How to train LSTM/neural networks](https://tesseract-ocr.github.io/tessdoc/tess5/TrainingTesseract-5.html)

#### Read
1. [Training @UB Mannheim](https://github.com/tesseract-ocr/tesstrain/wiki/GT4HistOCR#ocr-results)
2. [Train Tesseract Repository](https://github.com/tesseract-ocr/tesstrain)

#### Lessons Learned

**Tesseract limitations summed in the list.**
1. The OCR is not as accurate as some commercial solutions available to us.
2. Doesn&#39;t do well with images affected by artifacts including partial occlusion, distorted perspective, and complex background.
3. It is not capable of recognizing handwriting.
4. **It may find gibberish and report this as OCR output.**
5. If a document contains languages outside of those given in the -l LANG arguments, results may be poor.
6. **It is not always good at analyzing the natural reading order of documents. For example, it may fail to recognize that a document contains two columns, and may try to join text across columns.**
7. Poor quality scans may produce poor quality OCR.
8. It does not expose information about what font family text belongs to.

Quelle: [Tesseract OCR](https://nanonets.com/blog/ocr-with-tesseract/#training-tesseract-on-custom-data)

---

### 3. [Building Tesseract 5 from Source](https://www.youtube.com/watch?v=veJt3U44yqc)
**Vorteile:** Versionskontrolle, Funktionen modifizieren
**Achtung:** *These are the instructions for installing Tesseract from the git repository. You should be ready to face unexpected problems.*

1. Tesseract [GitHub-Repository](https://github.com/tesseract-ocr/tesseract) öffnen
2. Über die Table of Contents zu [Installing Tesseract](https://github.com/tesseract-ocr/tesseract#installing-tesseract) navigieren 
3. Über [Build Tesseract from Source](https://tesseract-ocr.github.io/tessdoc/Compiling.html) in die Dokumentation navigieren
4. Tesseract [GitHub-Repository](https://github.com/tesseract-ocr/tesseract) klonen ([Git installieren](https://github.com/git-guides/install-git), falls nötig [Linux Bash für Windows aktivieren](https://www.onlogic.com/company/io-hub/de/linux-bash-in-windows-10-aktivieren-so-geht-es/))
5. Installation-Guide folgen (Dependencies, Ubuntu, Leptonica)
6. Zu [Installing Tesseract from Git](https://tesseract-ocr.github.io/tessdoc/Compiling-%E2%80%93-GitInstallation.md) wechseln, um Trainingtools zu installieren
7. Befehle ab *To build Tesseract with training tools, run the following* durchführen


#### Fehlermeldungen:
- *Unable to find a valid copy of libtoolize or glibtoolize in your PATH* ([Issue #259](https://github.com/OCR-D/ocrd_all/issues/259)) beheben
- *Failed to retrieve available kernel versions* ([askubuntu](https://askubuntu.com/questions/1404129/ubuntu-22-04-lts-on-wsl-failed-to-retrieve-available-kernel-versions-failed)) beheben 

---

### 4. [Training Tesseract 5 for a new Font](https://www.youtube.com/watch?v=KE4xEzFGSU8)
([Github-Repository](https://github.com/astutejoe/tesseract_tutorial) zum Tutorial)

1. [README](https://github.com/tesseract-ocr/tesstrain/blob/main/README.md) zum Tesseract Training aufrufen
2. Zum Abschnitt *Provide Ground Truth* navigieren
3. Mithilfe des Python Skriptes [split_training_text.py](https://github.com/astutejoe/tesseract_tutorial/blob/main/split_training_text.py) mit text2image notwendige Dateien generieren und abspeichern
4. Dabei anzupassen: Sprache, Schriftart (.otf), sowie die Parameter: ysize (Höhe) und char_spacing (Abstand)
5. Ordner langdata mit deu.training_text befüllen
6. ``TESSDATA_PREFIX=../tesseract/tessdata make training MODEL_NAME=[insert model name] START_MODEL=[insert lang] TESSDATA=../tesseract/tessdata MAX_ITERATIONS=2000`` anpassen, um Training auszuführen 
7. Mit Anzahl der Wiederholung experimentieren ([Overfit](https://www.ibm.com/cloud/learn/overfitting) vermeiden, Errorrate verringern)
8. Model evaluieren: ``tesseract data/[insert ground truth folder]/[*].tif stdout --tessdata-dir/[*]/tesstrain/data --psm [insert psm] -l [insert model name] --loglevel ALL`` (notwendige Parameter anpassen)


#### Auswahl der Schriftart(en)
**1. Welche Schriftarten führen (möglicherweise) zu schlechterem Ouput?**
![](https://s3.hedgedoc.org/demo/uploads/2529a3a4-106a-4ba8-afe8-78d0f332464d.PNG)

Mögliche Schriftarten:
**1. Placard Next Round Bold by Monotype**
**2. Placard Next Round Compressed Bold by Monotype**
**3. Gill Sans Nova Condensed Ultra Bold by Monotype**
**4. Aachen SH Bold by Scangraphic Digital Type Collection**
5. Dance Lesson JNL by Jeff Levine
6. Pudgy Puss NF by Nick&#39;s Fonts
7. Typewriter Serial Extra Light by SoftMaker
8. Dirty Sundae Bold by Fenotype


**2. Welche verfügbaren Schriftart(en) repräsentieren Gemeinsamkeiten o.g. Schriftarten?**

Möglichkeit 1: Placard Next Round (Compression) Bold by [Monotype](https://www.monotype.com/de)
&gt; aus [Fonts for Tesseract Training](https://github.com/tesseract-ocr/langdata_lstm/blob/main/deu/okfonts.txt):
[...]
PistolShot LT Std Bold
Placard MT Std Bold
Placard MT Std Medium
Plantin Head MT Std Bold
[...] 

Zu MyFonts by Monotype: [Placard Next](https://www.myfonts.com/collections/placard-next-font-monotype-imaging)

Möglichkeit 2: Aachen SH Bold by [Scangraphic](https://www.myfonts.com/collections/scangraphic-digital-type-collection-foundry)
&gt; aus [Fonts for Tesseract Training](https://github.com/tesseract-ocr/langdata_lstm/blob/main/deu/okfonts.txt):
Aachen Std Bold
Aachen Std Medium
Abadi MT Std Bold
[...]

Möglichkeit 3: Gill Sans Nova Cond Bold
&gt; aus [Fonts for Tesseract Training](https://github.com/tesseract-ocr/langdata_lstm/blob/main/deu/okfonts.txt):
Gill Sans MT Std Medium Italic
Gill Sans Shadowed MT Std Light
Gill Sans WGL Bold
Gill Sans WGL Bold Italic
[...]

**3. Welche Schriftart für Training geeignet (=notwendiges Material gegeben)?**
Gill Sans Nova Cond Bold - Microsoft Font

#### Notwendiges Trainingsmaterial
1. Font als [.otf](https://fileinfo.com/extension/otf) Datei
2. Python Skript [split_training_text.py](https://github.com/astutejoe/tesseract_tutorial/blob/main/split_training_text.py) zur zeilenweisen Separation und Anwendung von [text2image](https://github.com/tesseract-ocr/tesseract/blob/main/doc/text2image.1.asc)
3. Ordner MODEL_NAME-ground-truth: .tiff, .box, .txt Dateien
4. Ordner tessdata: default Inhalt + [deu.traineddata](https://github.com/tesseract-ocr/tessdata_best/blob/main/deu.traineddata)
5. Ordner langdata_lstm: [deu](https://github.com/tesseract-ocr/langdata_lstm/tree/main/deu)

#### Read
1. [Tesseract Doku: Fonts](https://tesseract-ocr.github.io/tessdoc/Fonts.html)
2. [GitHub: Fonts for Tesseract Training](https://github.com/tesseract-ocr/langdata_lstm/blob/main/deu/okfonts.txt)
3. [GitHub: Best Trained Data Models (lang, script)](https://github.com/tesseract-ocr/tessdata_best)

[Einführung in die Implementierung](https://tesseract-ocr.github.io/tessdoc/tess4/NeuralNetsInTesseract4.00.html)

#### LSTM Neural Networks:
Da die Programme gängiger Texterkennungssoftware häufig auf dem Konzept des LSTM ([Long Short Term Memory](https://de.wikipedia.org/wiki/Long_short-term_memory)) aufbauen, ist es sinnvoll, dieses zumindest im Ansatz nachvollziehen zu können. Im Kontext des [überwachten maschinellen Lernen](https://de.wikipedia.org/wiki/%C3%9Cberwachtes_Lernen) macht sich das *lange Kurzzeitgedächtnis* darum die Technik der [Backpropagation](https://de.wikipedia.org/wiki/Backpropagation) zunutze. Auf diese Weise *vergessen* die künstlichen neuronalen Einheiten ([Units](https://en.wikipedia.org/wiki/Artificial_neural_network#Artificial_neurons)) Informationen nicht, wie bei herkömmlichen RNN, sondern können aufgrund eines kodierten und gewichteten *Erinnerungsvermögens* auf zurückliegende Informationen zugreifen und somit Lernfähigkeit und Resultate maßgeblich verbessern.

#### Read
- [Einführung in RNN &amp; LSTM](https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf)
- [Grundlagen LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Weiterführend: Transformer-Modelle](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0)

### 5. Training, what else?
1. **Workflow**: Blacklist mit Sonderzeichen &amp;rarr; @, ©, ®, $ etc. blacklisten, damit Gesamtoutput verbessern? (Encoding Error)
2. **Workflow**: Coverbild-Retusche &amp;rarr; Coverbilder ausblenden, damit Gesamtoutput verbessern?
3. **Workflow**: Finetune-Training für Cover-Bild Schriftarten &amp;rarr; Output für Coverbilder, damit Gesamtoutput verbessern?
4. **Workflow**: Text Detektion &amp;rarr; Segmentierung mit GUI (z.B. [von diesen](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html)) oder mit anderer Software (z.B.: [vielleicht so?](https://medium.com/technovators/scene-text-detection-in-python-with-east-and-craft-cbe03dda35d5) oder [hiermit?](https://arxiv.org/abs/1601.07140v2)) verbessern
5. **Workflow**: OCR mit OCR4all

#### [Blacklist](https://pyimagesearch.com/2021/09/06/whitelisting-and-blacklisting-characters-with-tesseract-and-python/) mit Sonderzeichen
tesseract: ```-l deu+Fraktur -c tessedit_char_blacklist=&#34;*,@,©,®,$&#34;```

Fehlermeldungen:
1. [utf-8-codec-cant-decode-byte-0xed](https://stackoverflow.com/questions/70044328/what-does-encoding-latin-1-do-when-reading-a-file): change ```encoding=&#39;utf8&#39;``` to ```encoding=&#39;latin1&#39;```
2. [IOPub data rate exceeded](https://stackoverflow.com/questions/65172293/how-to-solve-iopub-data-rate-exceeded-in-jupyter-notebook): run notebook by ```jupyter notebook --allow-root --NotebookApp.iopub_data_rate_limit=1.0e10```

#### [Tesseract GUIs](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html)
1. [Rescribe](https://rescribe.xyz/rescribe/): absolut user-unfreundlich
2. [Normcap](https://github.com/dynobo/normcap): screen capture tool
3. [VietOCR](https://vietocr.sourceforge.net/): see rescribe
4. [OCR2Text](https://github.com/writecrow/ocr2text):  PDF Input
5. [Tesseract4java](https://github.com/tesseract4java/tesseract4java): Fraktur
6. [ImageTrans](https://www.basiccat.org/imagetrans/): Comics

#### [OCR4all](https://www.ocr4all.org/)

1. [Docker installieren](https://docs.docker.com/desktop/install/windows-install/)
2. [OCR4all installieren](https://www.ocr4all.org/guide/setup-guide/windows)
3. Bilddateien intern konvertieren lassen (tif &amp;rarr; png)
4. Workflow für Stichprobe festlegen und ausführen
6. Ground Truth Production ([LAREX](https://www.uni-wuerzburg.de/fileadmin/10030600/Mitarbeiter/Reul_Christian/Projects/Layout_Analysis/LAREX_Quick_Guide.pdf))
7. optional: Training durchführen

**Workflow**
1. Preprocessing 
2. Noise Removal
3. Segmentation 
4. Line Segmentation
5. Recognition (geeignetes [Model](https://github.com/OCR4all/ocr4all_models))
6. Ground-Truth Production
7. Training (optional)

**LAREX-Editor Oberfläche**
![](https://s3.hedgedoc.org/demo/uploads/03a3e06b-b36e-4502-9f95-1d9fe021b8e7.png)

**Problem bei der Durchführung**
Einige der Seiten (Problemfälle: p16, p19, p29, p32, p37, p42, p54, p57) sorgen bei der Seiten- und Zeilensegmentierung für problematischen Output, da die Reihenfolge (auch manuell) nicht korrekt zugeordnet werden kann.

![](https://s3.hedgedoc.org/demo/uploads/0192e5a1-c3e6-484d-920a-a29afd6a8df2.png)


**Read**
- [OCR4all bei ForText](https://fortext.net/tools/tools/ocr4all)
- [OCR Introduction](https://www.ocr4all.org/guide/user-guide/introduction)
- [Systemanforderungen](https://docs.docker.com/desktop/install/windows-install/)
- [WSL2 installieren](https://learn.microsoft.com/en-us/windows/wsl/install)
- [Calamari-OCR Modelle](https://github.com/Calamari-OCR/calamari)


---

## Teil III: Vergleich, Nachbearbeitung &amp; Datenbank

---

### 1. Ergebnisse und Vergleich

#### Stichprobe
- ca. 10% (2 bis 4 x16)
- Auswahlkriterien: 4 (-Deckblatt/ Inhalt)
- Vergleich: *-gt.txt vs. *-out.txt
- Problem: Reihenfolge?
- Was ist das Minimum? 
    1. Titel
    2. Handlung
    3. Bestellinformationen
    4. extra: Creditangaben, Genre (wenn gegeben)
    5. extra: UFA Serie Information
    6. keine Bilder und Zusatzinformationen

**Fragen**
- Enthält Tesseract-Output die projektspezifisch notwendigen Zeilen?
- Besteht die Möglichkeit, die Reihenfolge der Zeilen miteinzubeziehen?
- Mit welchem Algorithmus lässt sich die Genauigkeit bestimmen?
- Wie lassen sich Line Matches aus GT Liste herausnehmen? 
- Anderer Ansatz: [Müll-Abgleich](https://www.python-forum.de/viewtopic.php?t=55371)? 

&amp;rarr; **pragmatisch:** keine Reihenfolge, SequenceMatcher als Vergleichswert

#### Skript
Das Python Skripte sowie Dokumentation und Datensätze zu finden unter: [GitHub super8_project](https://github.com/gitmthoma/super8_project)

**SequenceMatcher-Funktion (difflib Modul)**
Quelle: [SequenceMatcher in Python](https://towardsdatascience.com/sequencematcher-in-python-6b1e6f3915fc)

&gt; Given two input strings a and b,
&gt; 
&gt; - **ratio( )** returns the similarity score ( float in [0,1] ) between input strings. It sums the sizes of all matched sequences returned by function get_matching_blocks and calculates the ratio as: ratio = 2.0*M / T , where M = matches , T = total number of elements in both sequences
&gt; - **get_matching_blocks( )** return list of triples describing matching subsequences. The last triple is a dummy, (len(a), len(b), 0). It works by repeated application of find_longest_match( )
&gt; - **find_longest_match( )** returns a triple containing the longest matching block in a[aLow:aHigh] and b[bLow:bHigh]


#### Varianten im Stichprobenvergleich
1. **TIFF vs. JPG (BestStandard?)**
    tesseract: ```-l deu+Fraktur```
    TIF: **~91.34%**
    JPG: ~90.79%
2. **BestStandard vs. händische Retusche**
    BestStandard: **~91.34%**
    (händische) Retusche: ~84,36%
3. **BestStandard vs. OCR4allDefault**
    BestStandard:  **~91.34**
    OCR4All (default): ~91,2



#### Vergleichswert für Workflows bei UFA Katalogen 
- **Alternative**: zur Evaluation der Workflows (z.B. [OCR Evaluation with CER &amp; WER](https://towardsdatascience.com/evaluating-ocr-output-quality-with-character-error-rate-cer-and-word-error-rate-wer-853175297510))
- **Alternative**: zur Evaluation der Workflows Reihenfolge miteinbeziehen
- **Alternative**: gematchte Zeilen aus Output löschen (auskommentiert im Skript)

![](https://s3.hedgedoc.org/demo/uploads/4cd6456b-7d65-47f1-8370-672fa8a4e2f8.png)

![](https://s3.hedgedoc.org/demo/uploads/0e873ce3-0958-40aa-8b20-ccdfdf11f5e2.png)

![](https://s3.hedgedoc.org/demo/uploads/13081bf9-54ec-4e35-b3ab-af06beb87125.png)

![](https://s3.hedgedoc.org/demo/uploads/4b23acba-0e04-407b-874c-e094e6eb5659.png)

#### Auswertung
Das kommandozeilenbasierte Tesseract im Standardmodell bietet mit dem geringsten Arbeitsaufwand und einer überschaubaren Bildvorverarbeitung den besten Output. Vermutlich performt OCR4all bzgl. der Reihenfolge besser, was sich allerdings im Rahmen dieser Evaluation nicht auswerten ließ. Zudem scheint im Zusammenhang mit der Segmentierung ein bisher nicht behobener Fehler vorliegen, was die Zuordnung der Reihenfolge für den Output erheblich beeinflusst. Trainieren eines eigenen Modells (mit tesseract sowie OCR4all) wäre mit einem unverhältnismäßigen zeitlichen und/oder finanziellen Aufwand verbunden. 

---

## Könnte nützlich sein

1. über Kommandozeile alle [Einzel-txt-Files zu einem kopieren](https://adnanvatandas.wordpress.com/2010/09/26/texterkennung-mit-tesseract-windows/): ```copy /b *.txt Gesamttext.txt```
2. [Stapelweise Umbenennung](https://onlyhow.net/de/4-moglichkeiten-zum-batch-umbenennen-von-dateien-in-windows) von Dateien in einem Ordner über Kommandozeile/Windows Powershell: ```dir | %{$x=0} {Rename-Item $_ -NewName &#34;TestName$x.jpg&#34;; $x++ }```
3. [Linux Folder System Explained](https://averagelinuxuser.com/linux-root-folders-explained/)

---

![](https://licensebuttons.net/l/by/3.0/88x31.png)

Dieses Werk und dessen Inhalte (Text, Abbildungen sowie Befehle und Code) sind - sofern nicht anders angegeben - lizenziert unter [https://creativecommons.org/licenses/by/4.0/deed.de](https://creativecommons.org/licenses/by/4.0/deed.de).  

Nennung bitte wie folgt: *Projekt-Dokumentation: Workflows zur Texterkennung mit Tesseract und OCR4all am Beispiel von Super-8-Filmkatalogen des Anbieters UFA* von Merle-Sophie Thoma (2022), Lizenz: [https://creativecommons.org/licenses/by/4.0/deed.de](https://creativecommons.org/licenses/by/4.0/deed.de).