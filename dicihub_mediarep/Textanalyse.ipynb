{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Vermessung der medienwissenschaftlichen Welt?\n",
    "## Datengestützte Analysen mit media/rep/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Jupyter Notebook soll eine Grundlage bieten, die im Fachrepositorium media/rep/ vorhandenen Texte und die dazugehörigen Metadaten mithilfe eines Python-Skriptes zu analysieren. Hierzu werden Abschnitte für die Vorbereitung von Texten zur Analyse (Entfernung von Stoppwörtern, Neuzusammenstellung des Textkorpus, Gruppierung von Texten nach Jahreszahlen) sowie ein Durchsuchen der Texte nach konkreten Begriffen (als vollständige Wörter oder in Form einer Wildcard-Suche, Identifizierung von einzelnen Wortformen und -kombinationen bei einer Wildcard-Suche, Identifizierung von Einzeltexten, die einen Suchbegriff vermehrt enthalten) bereitgestellt. Funktional orientiert sich dieses Jupyter Notebook damit an Textanalysetools wie Voyant.\n",
    "  \n",
    "Generell eignet sich der bereit gestellte Code auch für die Analyse anderer Texte, die in demselben Format wie das media/rep/-Korpus vorliegen, wobei jedoch an einigen Stellen Anpassungen notwendig sein dürften.\n",
    "  \n",
    "Um das Notebook nutzen zu können, sind Grundkenntnisse in Python und im Umgang mit Jupyter Notebooks notwendig. Als ein Ergebnis aus der Zusammenarbeit von Medienwissenschaft und Digital Humanities ist das Notebook nicht daraufhin überarbeitet worden, eine möglichst hohe Performanz in der Umsetzung der Analyseschritte zu erreichen. Die Kommentierung der einzelnen Code-Abschnitte dient vor allem dazu, transparent zu machen, welche Schritte durchgeführt wurden, um die Analyseergebnisse zu erzielen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inhalt  \n",
    "1. [Vorbedingungen](#prerequisites)\n",
    "1. [Textpreprocessing](#preprocessing)  \n",
    "   a. [Eingrenzung des Korpus](#eingrenzung)    \n",
    "   b. [Gruppierung von Texten nach Publikationsjahr](#gruppierung)  \n",
    "   c. [Entfernen von Stoppwörtern](#stoppwörter)  \n",
    "2. [Textanalyse](#textanalyse)  \n",
    "   a. [Suche nach vollständigen Wörtern oder Wildcard-Suche](#ganzewörter)  \n",
    "   b. [Identifizierung von verschiedenen Wortformen und -kombinationen einzelner Suchbegriffe bei einer Wildcard-Suche](#wortformen)  \n",
    "   c. [Identifizierung von Einzeltexten, die einen Suchbegriff enthalten](#einzeltexte)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbedingungen <a class=\"anchor\" id=\"prerequisites\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analysegrundlage für dieses Jupyter Notebook stellen die im Fachrepositorium media/rep/ vorliegenden Publikationen dar. Diese können mithilfe eines Python-Skriptes im txt-Format heruntergeladen werden, wobei gleichzeitig eine Metadatentabelle generiert wird. Dieses Skript wird separat von diesem Jupyter Notebook zur Verfügung gestellt.\n",
    "   \n",
    "Für die Nutzung der in diesem Jupyter Notebook zur Verfügung gestellten Funktionalitäten ist es notwendig, die mittels des Skripts extrahierten txt-Dateien in einem Dateiordner zu speichern, der wiederum in einem übergeordneten Verzeichnis liegt, in der auch die bei Abruf der txt-Dateien automatisch gespeicherte Metadatentabelle sowie dieses Jupyter Notebook gespeichert werden. Hinzu kommt noch eine Stoppwortliste, die im Rahmen des Textpreprocessing verwendet wird und hier ebenfalls separat zur Verfügung gestellt wird. Diese vorgeschlagene Struktur gilt sowohl für die Nutzung des Jupyter Notebooks in einer IDE als auch einer Jupyter Umgebung. Die Ordnerstruktur sollte dementsprechend wie folgt aussehen:  \n",
    "  \n",
    "Hauptverzeichnis  \n",
    "-- > Dateiordner mit txt-Dateien  \n",
    "------- > 1988_....txt  \n",
    "------- > 2013_....txt  \n",
    "------- > ...  \n",
    "------- > 2020_....txt    \n",
    "-- > Metadatentabelle.csv  \n",
    "-- > Jupyter Notebook.ipynb  \n",
    "-- > Stoppwortliste.txt  \n",
    "  \n",
    "Die Benennung der aus media/rep/ extrahierten txt-Dateien sollte grundsätzlich beibehalten werden, da einige Schritte des Textpreprocessings an die automatisch generierten Dateinamen angepasst sind. Die Benennung des Dateiordners mit den txt-Dateien sowie die Benennung der Metadatentabelle sind frei wählbar und werden an gekennzeichneten Stellen in diesem Jupyter Notebook abgefragt, um die Analyseschritte darauf durchführen zu können.\n",
    "  \n",
    "Generell empfiehlt es sich, eine Kopie der aus media/rep/ extrahierten Texte zu behalten, da durch das Textpreprocessing Inhalte verändert werden können. Um den Zustand der einzelnen Zwischenschritte des Preprocessing zu speichern, werden in den nachfolgenden Skripten automatisch neue Dateiordner generiert, in dem die bearbeiteten Dateien gespeichert werden.\n",
    "  \n",
    "Der in diesem Jupyter Notebook zur Verfügung gestellte Code wurde mit den Python-Versionen 3.8 - 3.11 getestet.\n",
    "Zur Nutzung aller Funktionalitäten des Notebooks ist die Installation und der Import folgender Libraries notwendig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textbereinigung\n",
    "import re\n",
    "#Einlesen der txt-Dateien\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "#Erstellung von Dataframes und Berechnungen auf diesen\n",
    "import pandas as pd\n",
    "#Visualisierungen (Styling und Darstellung)\n",
    "import pygal \n",
    "from pygal.style import Style\n",
    "import cairosvg\n",
    "import lxml\n",
    "import tinycss2\n",
    "import cssselect2\n",
    "#Wortwolken\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Jupyter Notebook ist vom Prinzip her so aufgebaut, dass dem eigentlichen Skript jeweils eine Code-Zelle vorgelagert ist, in der beispielsweise Angaben zu dem Ordnernamen, der die zu analysierenden Dateien enthält, zu Suchbegriffen oder der Jahresrange der zu analysierenden Texte gemacht werden müssen.\n",
    "  \n",
    "Aufgrund der Größe des media/rep/-Textkorpus kann das Durchlaufen einer Code-Zelle je nach Bearbeitungsschritt bis zu 4 Minuten betragen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textpreprocessing <a class=\"anchor\" id=\"preprocessing\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eingrenzung des Korpus <a class=\"anchor\" id=\"eingrenzung\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Herunterladen von txt-Dateien aus media/rep/ lässt sich bereits über das oben erwähnte Skript den Analysebedürfnissen entsprechend anpassen, indem nur bestimmte Teile des Korpus extrahiert bzw. nicht extrahiert werden. Aber auch nach Herunterladen des Gesamtkorpus besteht die Möglichkeit, dieses in kleinere Teil-Korpora zu zerlegen, um gezielt Analysen auf beispielsweise allen publizierten Rezensionen oder auf allen Ausgaben einer spezifischen Zeitschrift eines bestimmten Zeitraums durchzuführen. Dies kann durch Auswählen oder Ausschließen eines bestimmten Teils des Korpus geschehen. In dem eingangs erwähnten Artikel wurde für die Analyse beispielsweise die Publikationsform PeriodicalPart nicht mitberücksichtigt, da darin einige Publikations-Items vorliegen, die gleichzeitig als review oder article geführt werden und somit doppelt in die Analyse eingeflossen wären. Dies gilt vereinzelt auch für book und bookPart.\n",
    "\n",
    "media/rep/ bietet unter anderem folgende Publikationsformen: review, article, PeriodicalPart, book, bookPart, workingPaper, doctoralThesis, report\n",
    "\n",
    "Des Weiteren sind verschiedene Publikationsreihen - sowohl Buch als auch Zeitschrift - vorhanden, die über eine ISSN-Nummer identifiziert werden können. Beispielsweise ist die Zeitschrift 'MEDIENwissenschaft: Rezensionen | Reviews' mit der 'issn:2196-4270' verknüpft. Einen Überblick über die Publikationsreihen und ihre jeweiligen ISSN-Nummern bietet die beim Herunterladen der txt-Dateien automatisch generierte Metadatentabelle.\n",
    "\n",
    "Als weitere Filtermöglichkeiten bietet sich zudem die Auswahl bestimmter Publikationsjahre oder Autor*innen an. Auch hierfür bietet die Metadatentabelle einen guten Überblick.\n",
    "\n",
    "Um die nachfolgenden Schritte durchführen zu können, dürfen die Dateinamen der aus media/rep/ extrahierten Dateien nicht nachträglich verändert worden sein, sondern müssen in ihrem Originalzustand belassen werden. Dies gilt auch für die in der Metadatentabelle aufgeführten Dateinamen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Einzufügen ist der Name der aus media/rep/ gewonnenen Metadaten-Datei, \n",
    "die dann in einen Dataframe eingelesen wird.\n",
    "'''\n",
    "metadata = \"\" #z.B. \"metadata.csv\"\n",
    "\n",
    "df_metadata = pd.read_csv(metadata, low_memory=False)\n",
    "\n",
    "'''\n",
    "Nachfolgend sind die verschiedenen Filteroptionen gelistet, \n",
    "die durch Entkommentieren ausgewählt werden können. \n",
    "Es besteht die Möglichkeit, Parameter auszuschließen (excluded) \n",
    "oder gezielt auszuwählen (included).\n",
    "Die ausgewählten Parameter können in die jeweils leere Liste, \n",
    "die .isin([ ]) folgt, eingetragen werden.\n",
    "Für jede Filteroption wird ein Beispiel gezeigt.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Option 1: Filtern nach Erscheinungsjahr\n",
    "Wählt oder schließt die Publikationen aus, \n",
    "die in den in der Liste eingetragenen Erscheinungsjahren publiziert wurden.\n",
    "Beispiel: \n",
    "years_excluded = df_metadata['dc.date.issued'].isin([\"2012\", \"2013\", \"2014\"]) == False \n",
    "schließt alle Publikationen aus, die in den genannten Jahren publiziert wurden.\n",
    "Das Erscheinungsjahr bezieht sich nicht auf die Publikation in media/rep/, \n",
    "sondern auf die Erstpublikation.\n",
    "Die ersten drei Code-Zeilen müssen unverändert übernommen werden. \n",
    "Da die in media/rep/ enthaltenen Jahresangaben \n",
    "unterschiedlichen Formaten folgen, müssen diese angeglichen werden.\n",
    "'''\n",
    "#years = df_metadata[\"dc.date.issued\"].to_list()\n",
    "#years = [i[:4] for i in years]\n",
    "#df_metadata[\"dc.date.issued\"] = years\n",
    "#years_excluded = df_metadata['dc.date.issued'].isin([]) == False\n",
    "#years_included = df_metadata['dc.date.issued'].isin([]) \n",
    "\n",
    "'''\n",
    "Option 2: Filtern nach Publikationsformat\n",
    "Wählt oder schließt die Publikationen aus, \n",
    "die in den in der Liste eingetragenen Publikationsformaten publiziert wurden.\n",
    "Beispiel: \n",
    "types_included = df_metadata['dc.type'].isin([\"article\", \"PeriodicalPart\"]) \n",
    "wählt alle Publikationen aus, \n",
    "die als article und PeriodicalPart publiziert wurden.\n",
    "'''\n",
    "#types_excluded = df_metadata['dc.type'].isin([]) == False\n",
    "#types_included = df_metadata['dc.type'].isin([])\n",
    "\n",
    "'''\n",
    "Option 3:\n",
    "Wählt oder schließt die Publikationen aus, \n",
    "die in den in der Liste eingetragenen Publikationsreihen publiziert wurden.\n",
    "Beispiel: \n",
    "isPartof_excluded = df_metadata['dc.relation.isPartOf'].isin([\"issn:2196-4270\"]) == False \n",
    "schließt alle Publikationen der Zeitschrift \"MEDIENwissenschaft: Rezensionen | Reviews\" aus.\n",
    "'''\n",
    "#isPartof_excluded = df_metadata['dc.relation.isPartOf'].isin([]) == False\n",
    "#isPartof_included = df_metadata['dc.relation.isPartOf'].isin([])\n",
    "\n",
    "'''\n",
    "Option 4:\n",
    "Wählt oder schließt die Publikationen aus, \n",
    "die von bestimmten Autor*innen verfasst wurden.\n",
    "Beispiel: \n",
    "creator_included = df_metadata['dc.creator'].isin([\"Kessler, Frank\"])\n",
    "wählt nur Publikationen aus, \n",
    "die von genanntem Autor verfasst wurden.\n",
    "'''\n",
    "#creator_excluded = df_metadata['dc.creator'].isin([]) == False\n",
    "#creator_included = df_metadata['dc.creator'].isin([])\n",
    "\n",
    "'''\n",
    "Um mehrere Filteroptionen gemeinsam nutzen zu können, \n",
    "müssen diese mit ihren oben genannten Variablennamen in der gewünschten \n",
    "Reihenfolge des Filterns in die nachstehende Liste eingetragen werden.\n",
    "Beispielsweise ließen sich somit zuerst alle article auswählen \n",
    "und in einem zweiten Schritt die Auswahl auf bestimmte Jahre eingrenzen.\n",
    "Die einzelnen Variablen müssen dabei mit dem & Zeichen verbunden werden.\n",
    "'''\n",
    "filter = #z.B. types_excluded & years_excluded\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Name des Ordners, \n",
    "in dem sich alle aus media/rep/ extrahierten Texte befinden.\n",
    "'''\n",
    "folder = \"\" #z.B. \"fulltext\"\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Name des Ordners, \n",
    "in dem die ausgewählten Dateien gespeichert werden sollen \n",
    "(dieser wird automatisch in diesem Verzeichnis generiert).\n",
    "'''\n",
    "filtered_folder = \"\" #z.B. \"filtered_corpus\"\n",
    "\n",
    "os.mkdir(filtered_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Der Dataframe wird nach den ausgewählten Optionen gefiltert.\n",
    "'''\n",
    "df_metadata_filtered = df_metadata.loc[filter]\n",
    "\n",
    "'''\n",
    "Die Dateinamen aller übrig gebliebenen Einträge im Dataframe \n",
    "werden in eine Liste überführt.\n",
    "'''\n",
    "filenames_list = df_metadata_filtered[\"fulltext_file\"].to_list()\n",
    "\n",
    "'''\n",
    "Die Dateinamen in der Liste werden mit den Dateinamen \n",
    "der im Ordner vorhandenen txt-Dateien abgeglichen.\n",
    "Wenn ein Dateiname im Ordner auf der Liste zu finden ist, \n",
    "wird diese Datei in den neu erstellten Ordner kopiert, \n",
    "der abschließend das neu erstellte Textkorpus enthält.\n",
    "'''\n",
    "for filename in os.listdir(folder):\n",
    "    if filename in filenames_list:\n",
    "        full_file_path = os.path.join(folder, filename)\n",
    "        shutil.copy((os.path.join(folder, filename)), filtered_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gruppierung von Texten nach Publikationsjahr <a class=\"anchor\" id=\"gruppierung\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Gesamtabzug des media/rep/-Korpus besteht aus einer Menge von über 18.000 Texten (Stand Januar 2023). Abhängig von den jeweiligen Analysefragen können diese Texte separat betrachtet werden. Insbesondere für eine Visualisierung von Ergebnissen ist es jedoch notwendig, diese Dateien zu gruppieren. Hierzu bietet sich eine Gruppierung nach Publikationsjahr an. Hierbei ist nicht das Jahr der Publikation in media/rep/, sondern das Jahr der Erstpublikation relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Einzufügen ist der Name des Ordners, \n",
    "in dem sich alle aus media/rep/ extrahierten Texte befinden.\n",
    "'''\n",
    "folder = \"\" #z.B. \"fulltext\"\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Name des Ordners, \n",
    "in dem die neu zusammengeführten Dateien gespeichert werden sollen \n",
    "(dieser wird automatisch in diesem Verzeichnis generiert).\n",
    "Es ist sinnvoll, diese nach Jahreszahlen gruppierten txt-Dateien \n",
    "in einem separaten Ordner zu speichern, \n",
    "da diese ansonsten inmitten der zehntausenden Dateien \n",
    "des bereits vorhandenen Ordners gespeichert würden \n",
    "und manuell herausgesucht werden müssten.\n",
    "'''\n",
    "year_folder = \"\" #z.B. \"fulltext_by_year\"\n",
    "\n",
    "os.mkdir(year_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Die vorhandenen Jahreszahlen werden \n",
    "aus den Dateinamen im gewählten Ordner extrahiert \n",
    "und in eine Liste überführt, \n",
    "wobei Duplikate entfernt \n",
    "und die Liste aufsteigend sortiert wird.\n",
    "'''\n",
    "years = [year for file in os.listdir(folder) for year in re.findall(\"(\\d{4})_\", file)]\n",
    "years = list(dict.fromkeys(years))\n",
    "years.sort()\n",
    "\n",
    "'''\n",
    "Die txt-Dateien, die zusammengeführt werden sollen, \n",
    "werden zusammen gemäß ihrem Publikationsjahr \n",
    "aus dem enstprechenden Ordner eingelesen.\n",
    "'''\n",
    "for year in years:\n",
    "    files = glob.glob(os.path.join(f\"{folder}\", f\"{year}_*.txt\"))                   \n",
    "    '''\n",
    "    Die nach Jahren eingelesenen Texte werden in dem neu erstellten Ordner\n",
    "    als eine gemeinsame txt-Datei pro Jahr erstellt.\n",
    "    '''\n",
    "    with open(os.path.join(f\"{year_folder}\", f\"{year}_fulltext.txt\"), \"wb\") as outfile:\n",
    "        for f in files:\n",
    "            with open(f, \"rb\") as infile:\n",
    "                outfile.write(infile.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entfernen von Stoppwörtern <a class=\"anchor\" id=\"stoppwörter\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um eine Textanalyse durchzuführen, bietet es sich an, vorab Stoppwörter zu entfernen, die für die Analyse nicht relevant sind (z.B. Präpositionen, Artikel, Füllwörter). Aufgrund der Größe des media/rep/-Korpus wird die inhaltliche Analyse der Texte durch das Entfernen von Stoppwörtern beschleunigt und erlaubt eine Fokussierung auf für das Textverständnis bedeutende Wörter. Dies ist insbesondere dann notwendig, wenn die inhaltliche Analyse das Ermitteln der häufigsten (semantisch bedeutsamen) Wörter beinhaltet. Im Rahmen der hier vorgestellten Analyse ist die Entfernung von Stoppwörtern zwar zu empfehlen, aber nicht zwingend erforderlich, da einer deduktiven Logik gefolgt und eine gezielte Suche nach Begriffen ermöglicht wird. Dies steht einem induktiven Ansatz gegenüber, bei dem durch Ermittlung von Worthäufigkeiten aller Wörter der Texte nach den häufigsten Begriffen gesucht wird. Letzteres stellt aufgrund der Größe des media/rep/-Korpus eine Herausforderung dar und erfordert eine zusätzliche Strategie, was den Umgang mit Stoppwörtern und der gemeinsamen Analyse verschiedener Wortformen angeht, und bietet sich somit für eine eigene Untersuchung an.\n",
    "  \n",
    "Zur Entfernung von Stoppwörtern kann auf eine im Kontext dieses Projektes erstellte Stoppwortliste zugegriffen werden, die auf der deutschen und englischen Stoppwortliste des Textanalysetools Voyant basiert und zusätzlich spezifisch dem media/rep/-Korpus angepasste Begriffe enthält.\n",
    "  \n",
    "Diese txt-Datei und somit die Auswahl und Anzahl der Stoppwörter kann je nach Bedarf angepasst werden. Vorraussetzung für die nachfolgenden Schritte ist, dass die Stoppwortliste in demselben Verzeichnis gespeichert ist wie dieses Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Einzufügen ist der Name der Stoppwortliste.\n",
    "'''\n",
    "stopword_list = \"\" #z.B. \"stopwords.txt\"\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Name des Ordners, \n",
    "in dem sich alle aus media/rep/ extrahierten Texte befinden.\n",
    "'''\n",
    "folder = \"\" #z.B. \"fulltext_by_year\"\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Name des Ordners, \n",
    "in dem die nach Stoppwörtern gefilterten Dateien gespeichert werden sollen \n",
    "(dieser wird automatisch in diesem Verzeichnis generiert).\n",
    "'''\n",
    "stopword_folder = \"\" #z.B. \"stopwordfiltered_by_year\"\n",
    "\n",
    "os.mkdir(stopword_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Einlesen der txt-Datei mit der Stoppworliste\n",
    "und Überführen der Stoppwörter in eine Liste.\n",
    "'''\n",
    "with open(stopword_list, \"r\", encoding=\"utf-8\") as infile1:\n",
    "    stopwords = [word.rstrip() for word in infile1]\n",
    "\n",
    "'''\n",
    "Jede im Dateiordner vorliegende txt-Datei \n",
    "wird nacheinander eingelesen \n",
    "und verschiedene Textbereinigungsschritte durchgeführt.\n",
    "'''\n",
    "for file in glob.glob(os.path.join(f\"{folder}\", \"*.txt\")):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as infile2:\n",
    "        text_stop = infile2.read()\n",
    "        '''\n",
    "        Am Ende einer Zeile durch Worttrennung \n",
    "        voneinander getrennte Wörter \n",
    "        werden zusammengeführt. \n",
    "        Zahlen, Zeilenumbrüche und Nicht-Wortzeichen \n",
    "        werden entfernt \n",
    "        und durch ein Leerzeichen ersetzt.\n",
    "        '''\n",
    "        text_stop = re.sub(\"-\\n+|\\d\", \"\", text_stop)\n",
    "        text_stop = re.sub(\"\\n|\\W+\", \" \", text_stop)\n",
    "        \n",
    "        '''\n",
    "        Die Stoppwörter werden aus den Texten entfernt, \n",
    "        indem die Wörter eines Textes \n",
    "        jeweils von einem String in eine Liste überführt \n",
    "        und die Stoppwörter aus dieser Liste herausgefiltert werden. \n",
    "        Anschließend wird die übriggebliebene Wortliste \n",
    "        eines jeden Textes wieder in einen String überführt. \n",
    "        ''' \n",
    "        text_stop = ' '.join([word for word in text_stop.split() if word.lower() not in stopwords])\n",
    "\n",
    "        '''\n",
    "        Die Dateinamen der eingelesenen Texte \n",
    "        werden auf ihren Kernnamen reduziert \n",
    "        (Dateierweiterung, Dateipfad wird entfernt).\n",
    "        '''\n",
    "        filename = Path(file).stem\n",
    "\n",
    "        '''\n",
    "        Die nach Stoppwörtern gefilterten Texte \n",
    "        werden in dem neu erstellten Ordner gespeichert.\n",
    "        '''\n",
    "        with open(os.path.join(f\"{stopword_folder}\", f\"{filename}.txt\"), \"w\", encoding=\"utf-8\") as outfile:\n",
    "            output_text = outfile.write(text_stop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textanalyse <a class=\"anchor\" id=\"textanalyse\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Abschnitt bezieht sich auf die Suche nach Wörtern in dem vorab erstellten Textkorpus und einer Visualisierung von Ergebnissen. Zum einen wird eine Suche nach vollständigen Wörtern bereitgestellt, zum anderen eine Wildcard-Suche, bei der lediglich der Anfang eines Wortes eingegeben werden muss und alle verschiedenen Endungsmöglichkeiten des Wortes in der Suche mitberücksichtigt werden. Die Ergebnisse werden in einen Dataframe überführt und können als csv-Datei gespeichert werden oder in Form eines Liniendiagramms betrachtet werden. \n",
    "  \n",
    "Um bei der Häufigkeitsanalyse die unterschiedliche Länge von Publikationen mitberücksichtigen zu können, werden relative statt absolute Zahlen in den Ergebnissen präsentiert. Dies bedeutet, dass die Häufigkeit des Vorkommens eines Suchbegriffes in einem Text in Relation zu der Gesamtzahl der Wörter eines Textes gesetzt wird. Abhängig davon, ob während des Textpreprocessing Stoppwörter aus den Texten entfernt wurden oder nicht, steht die ermittelte Anzahl der Wörter eines vorbearbeiteten Textes nicht in jedem Fall für die eigentliche Gesamtlänge eines Textes. Wurden die Stoppwörter vorab entfernt, sind die relativen Werte höher, als wenn sie in Bezug auf die Gesamtlänge eines unbereinigten Textes berechnet würden. Ein isoliert betrachteter Wert ist daher nicht aussagekräftig. Erst die Kontextualisierung und der Vergleich mit weiteren Werten lässt Schlussfolgerungen über die Häufigkeit der Suchbegriffe zu.\n",
    "  \n",
    "Um eine übersichtliche Ergebnismenge zu generieren, bedarf es insbesondere hier einer Gruppierung der ursprünglich mehr als 18.000 Texte aus media/rep/. Hierzu bietet sich die Gruppierung nach Jahren an, wobei insbesondere der Zeitraum zwischen 1980 und 2020 eine repräsentative Textbasis für die Analyse bietet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche nach vollständigen Wörtern oder Wildcard-Suche <a class=\"anchor\" id=\"ganzewörter\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In die Liste keywords können die Suchbegriffe eingefügt werden. \n",
    "Diese müssen kleingeschrieben werden.\n",
    "Es besteht die Möglichkeit einen vollständigen Begriff einzugeben, \n",
    "der in exakt dieser Form gesucht wird. \n",
    "Beispiel: \n",
    "Der Suchbegriff \"digital\" würde nicht die Vorkommen von \n",
    "\"digitale\", \"digitales\" etc. berücksichtigen, \n",
    "sondern nur den Begriff exakt wie er geschrieben wird.\n",
    "Es besteht aber auch die Möglichkeit \n",
    "nur die Anfangsbuchstaben eines Begriffes einzugeben, \n",
    "um verschiedene Wortformen und -kombinationen mitzuberücksichtigen \n",
    "Beispiel: \n",
    "Der Suchbegriff \"digital\" würde alle Wörter berücksichtigen, \n",
    "bei denen \"digital\" den Beginn des Wortes darstellt: \n",
    "\"digitales\", \"digitalität\", \"digitalpakt\" etc.\n",
    "Ob eine Suche nach vollständigen Wörtern \n",
    "oder eine Wildcard-Suche gewünscht ist, \n",
    "muss in der nachfolgenden Code-Zelle \n",
    "an der gekennzeichneten Stelle gewählt werden.\n",
    "Prinzipiell können so viele Suchbegriffe eingegeben werden, \n",
    "wie gewünscht ist. \n",
    "Zu viele Begriffe bedeuten jedoch auch eine größere Anzahl von Linien\n",
    "im erstellten Diagramm, was zu Unübersichtlichkeit führen kann. \n",
    "Es ist daher empfehlenswert, maximal 6 Suchbegriffe zu wählen.\n",
    "'''\n",
    "keywords = [] # z.B. [\"virtuell\", \"internet\", \"netz\", \"online\", \"interaktiv\", \"digital\"]\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Name des Dateiordners, \n",
    "der die zu analysierenden txt-Dateien enthält.\n",
    "'''\n",
    "folder = \"\" #z.B. stopwordfiltered_by_year\n",
    "\n",
    "'''\n",
    "Einzufügen ist die Range der untersuchten Texte als Index \n",
    "für den zu erstellenden Dataframe mit den Analyseergebnissen \n",
    "sowie für die Beschriftung der Visualisierung.\n",
    "Dies ist abhängig von der untersuchten Textgrundlage. \n",
    "Sind die Texte beispielsweise nach Jahren gruppiert \n",
    "und stammen aus den Jahren 1980-2020 \n",
    "ist die range(1980, 2021, 1) zu wählen \n",
    "(zur Erklärung der range-Funktion siehe\n",
    "https://docs.python.org/3/library/functions.html#func-range).\n",
    "'''\n",
    "range_texts = range(1980, 2021, 1)\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Dateiname für die Speicherung der Ergebnisse als csv-Datei\n",
    "'''\n",
    "name_csv = \"\" #z.B. \"suche_digit_virtu\"\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Dateiname für die Speicherung der Ergebnisdiagramme (png, svg)\n",
    "'''\n",
    "name_chart = \"\" #z.B. \"suche_digit_virtu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BITTE BEACHTEN: Für die Auswahl zwischen einer Suche nach vollständigen Wörtern und einer Wildcard-Suche müssen im folgenden Abschnitt an gekennzeichneter Stelle Code-Zeilen auskommentiert bzw. entkommentiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Anlegen eines leeren Dataframe, \n",
    "um die Ergebnisse der Häufigkeitsermittlung darin festzuhalten.\n",
    "'''\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "Anlegen einer leeren Liste, \n",
    "in der die Gesamtzahl der Wörter der einzelnen Texte gespeichert wird \n",
    "(relevant für die Berechnung von relativen Häufigkeiten).\n",
    "'''\n",
    "number_words = []\n",
    "\n",
    "'''\n",
    "Anlegen eines Dictionary, \n",
    "in dem die Suchbegriffe jeweils als key gespeichert werden \n",
    "und als value eine leere Liste angelegt wird, \n",
    "in der im Folgenden die Häufigkeit des Suchbegriffs \n",
    "in den einzelnen Texten eingefügt wird.\n",
    "'''\n",
    "frequency_keywords = {key:[] for key in keywords}\n",
    "\n",
    "'''\n",
    "Jede im Dateiordner vorliegende Textdatei \n",
    "wird nacheinander eingelesen \n",
    "und die Texte zur besseren Vergleichbarkeit \n",
    "vollständig in Kleinbuchstaben umgewandelt.\n",
    "'''\n",
    "for file in glob.glob(os.path.join(f\"{folder}\", \"*.txt\")):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        fulltext = infile.read().lower()\n",
    "\n",
    "        '''\n",
    "        Jeder Text wird von einem String in eine Wortliste umgewandelt, \n",
    "        wobei die Wörter an den Nicht-Wortzeichen gesplittet werden.\n",
    "        '''\n",
    "        fulltext_list = re.split(\"\\W+\", fulltext)\n",
    "\n",
    "        '''\n",
    "        Die Länge der jeweils erstellten Wortlisten gibt Auskunft über die Länge des Textes \n",
    "        und wird für jeden Text einer Liste hinzugefügt.\n",
    "        '''\n",
    "        number_words.append(len(fulltext_list))\n",
    "\n",
    "        '''\n",
    "        Erstellen eines Dictionary für jeden Text.\n",
    "        Hierzu wird über die Wortliste eines jeden Textes iteriert \n",
    "        und die gefundenen Suchbegriffe als keys dem Dictionary hinzugefügt. \n",
    "        Bei erstmaligem Vorkommen erhält der Suchbegriff 1 als value.\n",
    "        Taucht der Suchbegriff mehrfach im Text auf, \n",
    "        wird der value für jedes Vorkommen hochgezählt.\n",
    "        '''\n",
    "        fulltext_dict = {}\n",
    "        for i in fulltext_list:\n",
    "                if i in fulltext_dict:\n",
    "                    fulltext_dict[i] +=1\n",
    "                else:\n",
    "                    fulltext_dict[i] = 1\n",
    "        \n",
    "        '''\n",
    "        Aus den für die einzelnen Texte erstellen Dictionaries \n",
    "        werden nur die gesuchten Begriffe ausgewählt.\n",
    "        Die Anzahl des Vorkommens der einzelnen Suchbegriffe wird pro Text summiert. \n",
    "        Die summierten Ergebnisse werden in Form einer Liste \n",
    "        als value dem eingangs erstellten Dictionary hinzugefügt.\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        EINE DER BEIDEN FOLGENDEN OPTIONEN WÄHLEN UND DIE ANDERE AUSKOMMENTIEREN:\n",
    "        OPTION 1: SUCHE NACH VOLLSTÄNDIGEN WÖRTERN\n",
    "        '''\n",
    "        # for i in keywords:\n",
    "        #     keywords_dict = {k: v for k, v in fulltext_dict.items() if k == i}\n",
    "        #     frequency_keywords[i].append(sum(keywords_dict.values()))\n",
    "\n",
    "        '''\n",
    "        OPTION 2: WILDCARD_SUCHE\n",
    "        '''\n",
    "        # for i in keywords:\n",
    "        #    keywords_dict = {k: v for k, v in fulltext_dict.items() if k.startswith(i)}\n",
    "        #    frequency_keywords[i].append(sum(keywords_dict.values()))\n",
    "\n",
    "'''\n",
    "Die Ergebnisse aus dem Dictionary \n",
    "sowie die Anzahl der Wörter pro Text \n",
    "werden in den Dataframe eingefügt. \n",
    "Die eingangs definierte Text-Range \n",
    "dient als Index des Dataframe.\n",
    "'''\n",
    "df_results.index = range_texts\n",
    "for k,v in frequency_keywords.items():\n",
    "   df_results[k] = v\n",
    "df_results[\"words\"] = number_words\n",
    "\n",
    "'''\n",
    "Um relative Werte zu erhalten, \n",
    "wird die Anzahl des Vorkommens der keywords \n",
    "durch die Gesamtzahl der Wörter eines Textes dividiert.\n",
    "Für eine intiuitivere Erfassung der entstandenen Werte \n",
    "wird das Ergebnis mit 1000 multipliziert, \n",
    "so dass damit die Häufigkeit des Vorkommens eines Suchbegriffs \n",
    "pro 1000 Wörter Text dargestellt werden kann.\n",
    "Die Kolumne mit der Gesamtzahl der Wörter eines Textes \n",
    "wird anschließend gelöscht, \n",
    "da sie für die Erstellung der Visualisierung irrelevant ist.\n",
    "'''\n",
    "for column in df_results:\n",
    "    df_results[column] = df_results.loc[:,column].div(df_results.loc[:,\"words\"], axis = 0) * 1000\n",
    "df_results = df_results.drop(['words'], axis = 1)\n",
    "\n",
    "'''\n",
    "Speichern der Ergebnisse in einer csv-Datei\n",
    "'''\n",
    "df_results.to_csv(f\"{name_csv}.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusätzlich zur vorangehenden Code-Zelle kann die nachfolgende Code-Zelle ausgeführt werden, um die Ergebnisse in Form eines Liniendiagramms darzustellen oder zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Festlegen der Parameter für die Visualisierung \n",
    "(Farbe der Kurven, Schriftart und -größe für die Beschriftung des Diagramms, \n",
    "Strichstärke für Diagrammlinien)\n",
    "'''\n",
    "diagram_style = Style(\n",
    "    colors=('#FF0000', '#00FF00', '#0000FF', '#8A2BE2', '#000000', '#FFCC33'),\n",
    "    font_family='googlefont:Anuphan',\n",
    "    title_font_size=24,\n",
    "    label_font_size=13,\n",
    "    major_label_font_size=13,\n",
    "    guide_stroke_dasharray=0,\n",
    "    major_guide_stroke_dasharray=0)\n",
    "\n",
    "'''\n",
    "Erstellen eines Liniendiagramms aus den im Dataframe ermittelten Werten \n",
    "(für den Hintergrund der kubischen Interpolation \n",
    "siehe https://de.wikipedia.org/wiki/Spline-Interpolation)\n",
    "'''\n",
    "linechart_results = pygal.Line(style = diagram_style, x_label_rotation = 60, interpolate = \"cubic\")\n",
    "linechart_results.title = \"Häufigkeit des Vorkommens der analysierten Begriffe pro 1000 Wörter des Gesamttexts\"\n",
    "linechart_results.x_labels = range_texts\n",
    "for column in df_results:\n",
    "    linechart_results.add(f'{column}', df_results.loc[:,column])\n",
    "\n",
    "'''\n",
    "Darstellen des Liniendiagramms\n",
    "'''\n",
    "linechart_results\n",
    "\n",
    "\n",
    "#Zum Speichern des erstellten Diagramms als svg oder png \n",
    "#können die nachfolgenden Code-Zeilen entkommentiert werden.\n",
    "\n",
    "#linechart_results.render_to_file(f\"{name_chart}.svg\")\n",
    "#linechart_results.render_to_png(f\"{name_chart}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifizierung von verschiedenen Wortformen und -kombinationen einzelner Suchbegriffe bei einer Wildcard-Suche <a class=\"anchor\" id=\"wortformen\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die oben beschriebene Wildcard-Suche wird nicht ersichtlich, welche Wortformen und -kombinationen für die einzelnen Suchbegriffe berücksichtigt werden. Das nachfolgende Skript soll die Möglichkeit bieten, diese einzelnen Formen zu identifizieren, um eine stärkere Transparenz über die Ergebnisse der Wortzählung zu erzielen. Hierzu ist immer nur ein Suchbegriff auf einmal einzugeben (keine Liste von Suchbegriffen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Einzufügen ist das Suchwort, \n",
    "für das die ermittelten Einzelformen ausgegeben werden sollen. \n",
    "Beispiel:\n",
    "Die Suche nach \"postkol\" würde die Anzahl an Einzelformen \n",
    "wie Postkolonialismus, postkolonialistisch, postkolonial wiedergeben.\n",
    "'''\n",
    "keyword = \"\" #z.B. \"postkol\"\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Name des Dateiordners, \n",
    "der die zu analysierenden txt-Dateien enthält.\n",
    "'''\n",
    "folder = \"\" #z.B. stopwordfiltered_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Anlegen einer leeren Liste, \n",
    "in der die im Weiteren erstellten Dictionaries gesammelt werden.\n",
    "'''\n",
    "list_dict_keyword = []\n",
    "\n",
    "'''\n",
    "Jede im Dateiordner vorliegende txt-Datei wird nacheinander eingelesen\n",
    "und die Texte zur besseren Vergleichbarkeit vollständig in Kleinbuchstaben umgewandelt.\n",
    "'''\n",
    "for file in glob.glob(os.path.join(f\"{folder}\", \"*.txt\")):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        text_keyword = infile.read().lower()\n",
    "        \n",
    "        '''\n",
    "        Jeder Text wird von einem String in eine Wortliste umgewandelt, \n",
    "        wobei die Wörter an den Nicht-Wortzeichen gesplittet werden.\n",
    "        '''\n",
    "        text_keyword_list = re.split(\"\\W+\", text_keyword)\n",
    "\n",
    "        '''\n",
    "        Erstellen eines Dictionary für jeden Text.\n",
    "        Hierzu wird über die Wortliste eines jeden Textes iteriert \n",
    "        und die gefundenen Suchbegriffe als keys dem Dictionary hinzugefügt.\n",
    "        Bei erstmaligem Vorkommen erhält der Suchbegriff 1 als value.\n",
    "        Taucht der Suchbegriff mehrfach im Text auf, \n",
    "        wird der value für jedes Vorkommen hochgezählt.\n",
    "        '''\n",
    "        keyword_dict = {}\n",
    "        for i in text_keyword_list:\n",
    "                if i in keyword_dict:\n",
    "                    keyword_dict[i] +=1\n",
    "                else:\n",
    "                    keyword_dict[i] = 1\n",
    "        \n",
    "        '''\n",
    "        Aus den für die einzelnen Texte erstellen Dictionaries \n",
    "        werden nur die Wörter ausgewählt, \n",
    "        die mit dem als keyword ausgewählten Suchbegriff beginnen.\n",
    "        '''\n",
    "        keyword_dict = {k: v for k, v in keyword_dict.items() if k.startswith(keyword)}\n",
    "        \n",
    "        '''\n",
    "        Die für jeden Text entstandenen Dictionaries werden einer Liste hinzugefügt.\n",
    "        '''\n",
    "        list_dict_keyword.append(keyword_dict)\n",
    "\n",
    "'''\n",
    "Die in der Liste gesammelten einzelnen Dictionaries \n",
    "werden gemäß ihres keys (=der gefundenen Wortform) \n",
    "in einem gemeinsamen Dictionary zusammengefügt, \n",
    "wobei die Anzahl des Vorkommens der Wortform (= value des Dictionaries) \n",
    "in den einzelnen Texten zuerst als Liste gespeichert wird.\n",
    "'''\n",
    "keyword_dict_complete = {\n",
    "    k: [d.get(k) for d in list_dict_keyword if k in d]\n",
    "    for k in set().union(*list_dict_keyword)\n",
    "}\n",
    "\n",
    "'''\n",
    "Die in den values des Dictionary als Liste gespeicherten \n",
    "Einzelwerte des Vorkommens der einzelnen Wortformen werden summiert.\n",
    "'''\n",
    "for k,v in keyword_dict_complete.items():\n",
    "    keyword_dict_complete[k] = sum(v)\n",
    "\n",
    "'''\n",
    "Das Dictionary wird absteigend nach der Höhe der values sortiert \n",
    "(die häufigst vorkommenden Wortformen werden an den Anfang gestellt).\n",
    "'''\n",
    "keyword_dict_complete = {k: v for k, v in sorted(keyword_dict_complete.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "'''\n",
    "Das Dictionary wird in einen Dataframe überführt \n",
    "und das Ergebnis abschließend in einer csv-Datei gespeichert.\n",
    "'''\n",
    "df_keyword = pd.DataFrame.from_dict(keyword_dict_complete, orient='index', columns=[\"Häufigkeit\"])\n",
    "df_keyword.to_csv(f\"{keyword}_einzelformen.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis dieser Analyse lässt sich auch durch Nutzung des Ergebnis-Dictionary keyword_dict_complete und der Python-Library WordCloud in einer Wortwolke darstellen. Für die Funktionalitäten von WordCloud siehe auch https://www.python-lernen.de/wordcloud-erstellen-python.htm. Generell bietet sich diese Form der Visualisierung für verschiedene hier durchgeführte Textanalyseschritte an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Erstellen der WordCloud und Speichern als png\n",
    "'''\n",
    "wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(keyword_dict_complete)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"{keyword}_wortwolke.png\", facecolor='k', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifizierung von Einzeltexten, die einen Suchbegriff enthalten <a class=\"anchor\" id=\"einzeltexte\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das nachfolgende Skript soll die Möglichkeit bieten, die Anzahl und die Namen von Einzeltexten herauszufiltern, die einen bestimmten Suchbegriff enthalten. Während die vorangehenden Analyseschritte darauf basieren, dass die Texte nach Jahren gruppiert werden, um eine sinnvolle Auswertung zu ermöglichen, muss das nachfolgende Skript auf den aus media/rep/ extrahierten Einzeltexten ausgeführt werden. Hierzu ist prinzipiell keine Vorbearbeitung der Texte notwendig, es muss jedoch mit einer längeren Durchlaufzeit des Skriptes gerechnet werden (ca. 4-6 Minuten). Möchte man die Durchlaufzeit des Skriptes beschleunigen, kann das oben beschriebene Filtern von Stoppwörtern auch auf den Einzeldateien durchgeführt werden, was allerdings ebenfalls einen gewissen Zeitaufwand mit sich bringt. \n",
    "  \n",
    "Die nachfolgende Analyse stützt sich auf absolute im Gegensatz zu relativen Zahlen. Dies bedeutet, dass die Häufigkeit des Vorkommens des Suchbegriffes in einem Text nicht in Bezug zur Gesamtanzahl der Wörter des Textes gesetzt wird. Die Nutzung des Skriptes bietet sich an, wenn man gezielt nach Publikationen zu einem bestimmten Begriff sucht. \n",
    "  \n",
    "Das Skript sieht die Suche nach einem einzelnen Begriff vor und nicht die Suche nach mehreren Begriffen auf einmal. Hierzu müsste das Skript mehrmals hintereinander mit jeweils einem neuen Suchbegriff durchlaufen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Eingabe des Suchbegriffes, \n",
    "für den das Vorkommen in Einzeltexten ermittelt wird.\n",
    "Mit dieser Suche kann nicht nur nach vollständigen Wörtern gesucht werden. \n",
    "Es werden auch Wörter wiedergegeben, \n",
    "die mit dem eingegegeben Suchbegriff beginnen, \n",
    "aber unterschiedliche Endungen aufweisen \n",
    "Beispiel:\n",
    "Für \"digital\" wird auch \"digitalität\", \"digitales\", \"digitally\" wiedergegeben.\n",
    "''' \n",
    "keyword_singletexts = \"\" #z.B. \"virtuell\"\n",
    "\n",
    "'''\n",
    "Einzufügen ist der Name des Dateiordners, \n",
    "der die zu analysierenden txt-Dateien enthält. \n",
    "In diesem Ordner müssen die aus media/rep/ extrahierten Einzeldateien vorliegen\n",
    "und keine nach Jahren gruppierten Texten.\n",
    "'''\n",
    "folder = \"\" #z.B. \"fulltext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Anlegen eines leeren Dataframe, \n",
    "um die Ergebnisse der Häufigkeitsermittlung festzuhalten.\n",
    "'''\n",
    "df_singletexts = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "Anlegen einer leeren Liste, \n",
    "in der die Namen der Einzeltexte gespeichert werden, \n",
    "die als Index des erstellten Dataframe fungieren.\n",
    "'''\n",
    "filename_singletexts_list =  []\n",
    "\n",
    "'''\n",
    "Anlegen einer leeren Liste, \n",
    "in der die im Weiteren erstellten Dictionaries gesammelt werden.\n",
    "'''\n",
    "list_singletexts_keyword = []\n",
    "\n",
    "'''\n",
    "Jede im Dateiordner vorliegende txt-Datei wird nacheinander eingelesen \n",
    "und die Texte zur besseren Vergleichbarkeit \n",
    "vollständig in Kleinbuchstaben umgewandelt.\n",
    "'''\n",
    "for file in glob.glob(os.path.join(f\"{folder}\", \"*.txt\")):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        text_singletexts = infile.read().lower()\n",
    "\n",
    "        '''\n",
    "        Jeder Text wird von einem String in eine Wortliste umgewandelt, \n",
    "        wobei die Wörter an den Nicht-Wortzeichen gesplittet werden.\n",
    "        '''\n",
    "        text_singletexts_list = re.split(\"\\W+\", text_singletexts)\n",
    "\n",
    "        '''\n",
    "        Die Dateinamen der eingelesenen Texte \n",
    "        werden auf ihren Kernnamen reduziert \n",
    "        (Dateierweiterung, Dateipfad wird entfernt) \n",
    "        und in einer Liste gesammelt, \n",
    "        die im Weiteren den Index des Dataframes darstellt.\n",
    "        '''\n",
    "        filename_singletexts = Path(file).stem\n",
    "        filename_singletexts_list.append(filename_singletexts)\n",
    "\n",
    "        '''\n",
    "        Erstellen eines Dictionary für jeden Text.\n",
    "        Hierzu wird über die Wortliste eines jeden Textes iteriert \n",
    "        und die gefundenen Suchbegriffe als keys dem Dictionary hinzugefügt.\n",
    "        Bei erstmaligem Vorkommen erhält der Suchbegriff 1 als value.\n",
    "        Taucht der Suchbegriff mehrfach im Text auf, \n",
    "        wird der value für jedes Vorkommen hochgezählt.\n",
    "        '''\n",
    "        singletexts_dict = {}\n",
    "        for i in text_singletexts_list:\n",
    "                if i in singletexts_dict:\n",
    "                    singletexts_dict[i] +=1\n",
    "                else:\n",
    "                    singletexts_dict[i] = 1\n",
    "        \n",
    "        '''\n",
    "        Aus den für die einzelnen Texte erstellen Dictionaries \n",
    "        werden nur die Wörter ausgewählt, \n",
    "        die mit den als keywords eingetragenen Suchbegriffen beginnen.\n",
    "        Die Anzahl des Vorkommens der einzelnen Wortformen und -kombinationen \n",
    "        zu einem Suchbegriff wird addiert. \n",
    "        Das Ergebnis wird einer Liste hinzugefügt.\n",
    "        '''\n",
    "        keyword_singletexts_dict = {k: v for k, v in singletexts_dict.items() if k.startswith(keyword_singletexts)}\n",
    "        list_singletexts_keyword.append(sum(keyword_singletexts_dict.values()))\n",
    "\n",
    "'''\n",
    "Die Ergebnisse aus dem Dictionary sowie die Anzahl der Wörter pro Text \n",
    "werden in den Dataframe eingefügt.\n",
    "'''\n",
    "df_singletexts.index = filename_singletexts_list\n",
    "df_singletexts[keyword_singletexts] = list_singletexts_keyword\n",
    "\n",
    "'''\n",
    "Ausgabe der Anzahl der Texte des Gesamtkorpus, \n",
    "die den Suchbegriff enthalten, sowie den Prozentanteil, \n",
    "den diese Texte vom Gesamtkorpus ausmachen.\n",
    "'''\n",
    "text_with_content = len(list_singletexts_keyword) - list_singletexts_keyword.count(0)\n",
    "percent_content = round(text_with_content / len(list_singletexts_keyword) * 100, 2)\n",
    "print(f\"Anzahl der Texte, die den Suchbegriff '{keyword_singletexts}' enthalten: {text_with_content} (in Prozent: {percent_content}%) \\n\")\n",
    "\n",
    "'''\n",
    "Gibt die 10 häufigsten Textnamen \n",
    "und die Häufigkeit der Nennung in absoluten Zahlen der Texte aus, \n",
    "die den Suchbegriff am häufigsten enthalten\n",
    "'''\n",
    "print(f\"Nachfolgend sind die 10 Texte gelistet, in denen der Suchbegriff '{keyword_singletexts}' in absoluten Zahlen am häufigsten vorkommt: \\n\")\n",
    "\n",
    "print(df_singletexts[keyword_singletexts].nlargest(10).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
